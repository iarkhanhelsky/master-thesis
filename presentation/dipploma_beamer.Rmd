---
title: "Анализ изображений на основе веййвлетов Добеши"
author: "Архангельский Илья Андреевич"
output:
  beamer_presentation:
    includes:
      in_header: ../src/style/header_beamer.tex
    incremental: yes
    keep_tex: yes
    toc: yes
  ioslides_presentation:
    incremental: yes
  slidy_presentation:
    incremental: yes
---
```{r, echo=FALSE, message=FALSE}
source('../src/r/helpers/visual.r')
source('../src/r/statistics.r')
source('../src/r/wavelets.r')
source('../src/r/denoising.r')
require(ggplot2)
```


##План

- Введение
- Вейвлет. Вейвлет преобразование
- Дискретное вейвлет преобразование. Алгоритм Маллата.
- Дискретное вейвлет преобразование в двумерном случае.
- Постановка задачи фильтрации сигнала.
- Пороговые вейвлет методы шумоподавления
- SureShrink
- NeighBlock


## Введение

Преобразование Фурье является достаточно популярным и хорошо изученным инструментом в сфере обработки сигналов и анализа временных рядов. Однако:

- из анализируемого сигнала $x(t)$ невозможно напрямую получить информацию описывающую его частотную характеристику
- хотя разложение $X(f)$ полученное с помощью преобразовнаия Фурье сингнала $x(t)$ содержит в себе всю информацию о сигнале в частотном пространстве, нет ни какой информации о том где конкретно во времени та или иная частота появилась. Иными словами ни $x(t)$, ни $X(f)$ не дают полного описания сигнала.

Альтернативой не обладающей такими недостатками является вейвлет-преобразование.

----

При решении задач фильтрации дискретное вейвлет преобразование дает неоспоримое преимущество:

> Обработка сигнала в вейвлет пространстве позволяет легко отделиить зашумленные данные.


## Вейвлет. Вейвлет преобразование

## Пример вейвлет разложения модельных данных
```{r decomp, echo=FALSE, message=FALSE}

d <- BJsales[1:128]
vis.decomp(d, filter='d4', 1, 1)

```

## Каскадный алгоритм Маллата
\begin{figure}[h!]
\centering
    \includegraphics{../src/res/1ddecomp.png}
\caption{Иллюстрация схемы двумерного вейвлет разложения}\label{fig:decompex}
\end{figure}

----

В двумерном случае дискретное вейвлет преобразование так же рассматривается как свертка вейвлет функции и исходного изображения, однако может быть равнозначно представлена как набор из двух матриц-фильтров (один для строк, другой для столбцов).

\begin{equation}
\textbf{C}=\textbf{X}\cdot \textbf{I} \cdot \textbf{Y}
\end{equation}
\noindent
где $\textbf{C}$ -- результирующая матрица вейвлет коэффициентов, $\textbf{I

----

\begin{figure}[h!]
\centering
    \includegraphics{../src/res/decomp.pdf}
\caption{Иллюстрация схемы двумерного вейвлет разложения}\label{fig:2ddecomp}
\end{figure}

## Постановка задачи фильтрации сигнала

Пусть имеется выборка зашумленных значений некоторой функции $f$ размера $n$:

\begin{equation}\label{eq:noiseq}
    y_i = f(t_i) + \sigma \varepsilon_i, ~ i = 1 \dots n
\end{equation}

где $\varepsilon_i$ независимые одинаково распределенные с законом ${N}(0,~1)$ случайные величины, а уровень шума $\sigma$ может быть неизвестен.

----

 Основной задачей является выделение значений исходной функции $f$ из зашумленной выборки $y = (y_1, \dots, y_n)$ с минимальной ошибкой, где мерой ошибки выбирается среднеквадратичное отклонение, иными словами необходимо найти такую функцию $\hat{f}$, которая удовлетворяет следующему условию:

\begin{equation}\label{eq:noisesolve}
\hat{f} = \underset{f^*}{min} || f^* - f ||_2
\end{equation}

где $\hat{f} = \hat{f}(y)$. Должно быть понятно, что на практике функция $f$ неизвестна, так что используется оценка среднеквадратической ошибки.
----

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{../src/res/mri.jpg}
  \center{а)}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{../src/res/mri_noise.jpg}
  \center{б)}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{../src/res/mri_denoise.jpg}
  \center{в)}
\endminipage
\caption{а) Исходное изображение МРТ мозга б) Исходное изображение МРТ мозга с наложенным шумом в) Результат шумоподавления с помощью фильтра Гаусса}\label{img:brain}
\end{figure}

### Пороговые вейвлет методы

В вейвлет пространстве условие задачи перепишется как:

\begin{equation}\label{eq:wavenoise}
    d_{jk} = \omega_{jk} + \sigma \tilde{\varepsilon}_{jk}
\end{equation}


где $\omega_{jk}$ представляют собой ``чистые вейвлет коэффициенты'' (преобразования исходной функции $f(t_i)$), а $\tilde{\varepsilon}_{jk}$ являются независимыми одинаково распределенными величинами с законом распределения $\mathcal{N}(0,~1)$.

----

- Коэффициенты вейвлет преобразования обычно являются разреженными. 
- Это значит, что большая часть из вейвлет коэффициентов чистого сигнала достаточно близка к нулю.
- Таким образом, мы можем переформулировать исходную задачу восстановления исходной функции $f$, как задачу восстановления вейвлет коэффициентов функции $f$, которые относительно ``устойчивее'' чем белый гауссовый шум. 
-Тем самым, коэффициенты с достаточно малыми значениями, могут рассматриваться как чистый шум и должны быть установлены в ноль.

----

Обычно используют 2 типа замены вейвлет коэффициентов(жесткий и мягкий) 

$$
\delta^H_{\lambda} (d_{jk}) = \begin{cases}
    0, ~ |d_{jk}| \leq \lambda \\
    d_{jk}, ~ |d_{jk}| > \lambda
\end{cases}
$$


$$
\delta^S_{\lambda} (d_{jk}) = \begin{cases}
    0, ~ |d_{jk}| \leq \lambda \\
    d_{jk} - \lambda, ~ d_{jk} > \lambda \\
    d_{jk} + \lambda, ~ d_{jk} < -\lambda \\
\end{cases}
$$

----

```{r hardsoft, echo=FALSE, message=TRUE, fig.cap="Иллюстрация жесткой и мягкой замены вейвлет коэффициентов соотвественно $(\\lambda = 2)$", out.extra='', dev='png', dpi=128}

lambda <- 2
bounds <- 10
vis.thresholding.types(lambda, bounds)
```
----
Дисперсия шума на практике зачастую не известна.

Широко распространенной оценкой уровня шума $\sigma$ является оценка основывающаяся на последнем уровне детализации вейвлет разложения, следуя медианному значению абсолютного отклонения:

\begin{equation}
    \hat{\sigma} = \frac{m(|d_{J-1, k}|)_{k = 0}^{2^{J-1}-1}}{0.6745}
\end{equation}

где число в знаменателе является множителем мастшаба, который вообще говоря зависит от распределения коэффициентов $d_{jk}$ и равен $0.6745$ в случае нормально распределенных данных. Для простоты будем полагать, что дисперсия шума известна $\sigma$ и равна 1.

## SureShrink

### Оценка порогового значения
Пусть величины $X_1, \dots, X_s$ независимы и одинаково распределены с нормальным законом $\mathcal{N} (\mu_i, ~ 1), ~ (i = 1, \dots, s)$. Задача состоит в том, чтобы оценить вектор средних $\mu = (\mu_1, \dots \mu_s)$ из известной выборки $\mathbf{x} = (X_1, \dots, X_s)$ с мимимальным риском. Иными словами необходимо найти оценку, которая удовлетворяет

\begin{equation}
    \hat{\mu} = \underset{\hat{\mu}}{min} || \mu - \hat{\mu} ||_2
\end{equation}

----

Точное значение риска на практике не известно, так как истинное значение $\mu$ неизвестно. Для того чтобы получить оценку риска без необходимости знать истинное значение $\mu$ используем наблюдение, что для каждой оценки $\mu$, которая может быть записана как $\hat{\mu} = \mathbf{x} + g(\mathbf{x})$, где $g : \mathbb{R}^s \rightarrow \mathbb{R}^s$ и $g$ слабо дифференцируема, тогда риск может быть оценен как:

\begin{equation}\label{eq:sure}
    SURE(\hat{\mu}) = E_{\mu} || \mu - \hat{\mu}||_2^2 = s + E_\mu[||g(\mathbf{x})||_2^2 + 2\nabla * g(\mathbf{x})]
\end{equation}

где $\nabla g(t)$ -- дивергенция $g$:

----

Положим $\omega_{jk}$ как $\mathbf{x}$. Используя выражение для мягкого порогового правила $\delta^S_{\lambda}$, можно заметить, что $\delta^S_{\lambda} (\mathbf{x}) = x + g(\mathbf{x})$, где:

\begin{equation}
    ||g(\mathbf{x})||_2^2 = \sum_{i = 1}^s [min(|X_i|, \lambda)]^2
\end{equation}

\begin{equation}
    \dot g(\mathbf{x}) = \sum_{i=1}^{s} 1_{[-\lambda, \lambda]}(X_i)
\end{equation}

($1_A(x)$ -- понимается в смысле индикаторной функции на множестве $A$). Теперь возможно получить явное выражение для $SURE$:

\begin{equation}\label{eq:sureex}
    SURE(\lambda, \mathbf{x}) = s + \sum_{i = 1}^s [min(|X_i|, \lambda)]^2 - 2 \cdot \# \{i: |X_i| < \lambda\}
\end{equation}

----

\begin{equation}\label{eq:suremin}
    \lambda^* = \underset{0 \leq \lambda \leq \sqrt{2\log s}}{argmin} SURE(\lambda; \mathsf{\mathbf{x})}
\end{equation}

следует заметить, что при поиске оптимального порогового значения $\lambda$ не принимаются во внимание значения, которые больше чем $\lambda_U = \sqrt{2\log s}$ (которое назовем __универсальной границей__)

----

```{r lambdau, echo=FALSE, message=FALSE, fig.cap="Универсальное пороговое значение расчитанноне по выборке из N(0, 1) размером 10000", dev='png', dpi=128, out.extra=''}
vis.universal.bound(10000)
```

----

Показано что, если $X_1, \dots, X_s$ независимы и одинаково распределены с нормальным законом распределения $N(0, ~ 1)$, тогда:

\begin{equation}\label{eq:sureprob}
    P\{max_{1 \leq i \leq s} |X_i| > \sqrt{2\log s}\} \mathtt{\sim} \frac{1}{\sqrt{\pi \log s }} \underset{s \rightarrow \infty}{\rightarrow} 0
\end{equation}

----

Однако, такой выбор $\lambda^*$ не всегда является оптимальным. Это происходит из-за значительной разреженности вейвелет коэффициентов. Тогда предлагается смешанная схема, которую называют __SureShrink__.

Следуя идеологии метода, пороговое значение в разреженном случае должно быть установлено равным универсальному граничному значению $\lambda_U$, если вейвлет коэффициенты не достаточно разрежены, предполагается что SURE дает хорошую оценку потерь, и используется $\lambda^*$

----

Желая определить какое из значений нам необходимо использовать, мы вводим __меру разреженности__ вейвлет коэффициентов введем следующую меру:

\begin{equation}\label{eq:sparsity}
 \nu_s(\boldsymbol x) = \frac{s^{-1}\sum_{i=1}^{s}(x^2_i - 1)}{s^{-1/2}\log_2^{3/2}(s)}
 = s^{-1/2} \frac{\sum_{i=1}^{s}(x^2_i-1)}{\log_2^{3/2}(s)}
\end{equation}

----

Окончательно, правило выбора порогового значения для зашумленных данных используя SureShrink определяется следующим образом:

\begin{equation}\label{eq:sureshrink}
\lambda_{SureShrink}(\mathbf{x}) = \begin{cases}
  \sqrt{2\log s}, ~ \nu_s(\mathbf{x}) \leq 1 \\
  \lambda^*, ~ \nu_s{\mathbf{x}} > 1
  \end{cases}
\end{equation}


----



----


## Сравнение результатов работы методов 


```{r sureRes, echo=FALSE, fig.cap='Результат работы метода SureShrink. Серым обозначен выделенный шум. Красным исходный сигнал. Голубым -- восстановленный сигнал.'}
t <- seq(0, 1, length.out=1024)
x <- sin(10* pi * t) * exp(cos(t))
n <- rnorm(1024) * 0.05

vis.diff(t, x , sure.shrink(n + x, 'd8')) + geom_line(aes(t, n), color='gray') + theme(legend.position = 'none')
```

----

```{r neighRes, echo=FALSE, fig.cap='Результат работы метода NeighBlock. Серым обозначен выделенный шум. Красным исходный сигнал. Голубым -- восстановленный сигнал.'}
vis.diff(t, x, neigh.block(n + x, 'd8')) + geom_line(aes(t, n), color='gray')  + theme(legend.position = 'none')
```

В случае NeighBlock среднеквадратическая ошибка в среднем на 15% ниже чем в случае SureShrink при использовании одного и того же фильтра и одинакового числа уровней.



