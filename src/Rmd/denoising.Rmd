
# Вейвлет анализ в задачах фильтрации
Достаточно известный факт, что в на практике редко приходитя работать с данными без шума, который, однако, может быть незначительным (в смысле отношения сигнал/шум) в некоторых условиях. Тем не менее, достаточно часто шум повреждает сигнал в значительной мере и должен быть устранен из данных для обеспечения возможности проведения дальнейшего анализа. Процесс устранения шума из сигнала в общем случае называется __фильтрацией__ (или __шумоподавлением__). Примеры чистого и зашумленнонр сигналов привелены на рисунке[@nosieex]. Можно заметить, что шум добавляет высокочастотные компоненты в изначально достаточно гладки сигнал. Это является характеристической особенностью шума.


```{r, echo=FALSE, fig.cap="Примеры зашумленного и оригинального сигнала"}
require(reshape2)
require(ggplot2)
t <- seq(0, 3*pi, 0.01)
data <- data.frame(
  noised = 3*sin(t) + rnorm(length(t)),
  clean = 3*sin(t),
  t = t)
data.long <- melt(data, id='t', value.name='x')

ggplot(data=data.long, aes(x=t, y=x, colour=variable)) +
  theme(legend.position="none") +
  geom_line()
```

Не смотря на то, что понятие ``фильтрация'' достаточно общеее, оно обычно относится к восстановлению сигнала, который был смешан с аддитивным белым Гауссовым шумом, нежели с какими-либо другими видами шумов (такими как не-аддитивный шум, шумы Пуассона, Лапласса и прочие). В этой главе мы сосредоточимся на случае, когда сигнал подвержен воздействию белого шума.

Оптимизационный критерий, следуя которому измеряется эффективность алгоритмов шумоподавления, обычно выбирается в смысле среднеквадратической ошибки, между оригинальным сигналом (если он существует) и его восстановленной версией. Этот общий критерий выбирается из соображений простоты вычисления. Более того, обычно приводит к аналитически легко вычисляемым выражениям. Однако, он может быть недопустимым в некоторых задачах, которых точность восстановления сигнала является определяющей, хотя само понятие точности является спорным, особенно в отсуствии оригинального сигнала.

Существует довольно много приложений, где подавление шума играет важную роль. Хорошим примером могут быть анализ изображений и сигналов в медицинских ислледованиях, радиоастрономии, датамайнинге и прочих. В каждой конкретной области имеются свои специальные требования. Например устранение шума в медицине требует особой точности и аккуратности, так как шумоподавление использующее сглаживание зашумленного сигнала (то есть используя НЧ-фильтр[^filters]) может привести к потере точных и важных деталей. Как это показано на \ref{img:brain}

[^filters]:
  Фильтр нижних частот (ФНЧ) — фильтр, эффективно пропускающий частотный спектр сигнала ниже некоторой частоты (частоты среза), и уменьшающий (подавляющий) частоты сигнала выше этой частоты.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{../src/res/mri.jpg}
  \center{а)}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{../src/res/mri_noise.jpg}
  \center{б)}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{../src/res/mri_denoise.jpg}
  \center{в)}
\endminipage
\caption{а) Исходное изображение МРТ мозга б) Исходное изображение МРТ мозга с наложенным шумом в) Результат шумоподавления с помощью фильтра Гаусса}\label{img:brain}
\end{figure}


Следсвием острой необходимости в качественных методах устранения шума является достаточно большое количество подходов к решению задачи фильтрации, которые в свою очередь могут быть разделены на две категории:

* фильтрация происходит в пространстве исходного сигнала (то есть время или пространство)
* фильтрация происходит в пространстве примененного преобразования (например преобразования Фурье)

Активное развитие теории вейвлет-анализа и вейвлет-преобразований, происходящая в последние два десятилетия революционным образом изменила подходы применяемые в обработке сигналов и изображений, особенно в области шумоподавления. Начиная с 1990-ых, в этой области преобладающими методами были __вейвлет сжатие(shrinkage)__ и __пороговый вейвлет метод__ (подробнее о них будет рассказано позже)[@donobo1][@donobo2][@donobo3][@donobo4][@donobo5].
Обзор этих подходов дан в работе [@six].

В этой главе представлены и реализованы два метода шумоподавления. Первый -- __SureShrink__, представлен в работе Донобо и Йохонстона [@donobo3]. Второй был представлен Каи и Сильверманом в [@silver7], и называется __NeighBlock__.

## Каскадный алгоритм Маллата

## Постановка задачи

Распространенная формулировка задачи фильтрации звучит следующим образом. Пусть имеется выборка зашумленных значений некоторой функции $f$ размера $n$:

\begin{equation}\label{eq:noiseq}
    y_i = f(t_i) + \sigma \varepsilon_i, ~ i = 1 \dots n
\end{equation}

где $\varepsilon_i$ независимые одинаково распределенные с законом ${N}(0,~1)$ случайные величины, а уровень шума $\sigma$ может быть неизвестен. Пример такой выборки приведен на Рисунке \ref{fig:snr}. Основной задачей является выделение значений исходной функции $f$ из зашумленной выборки $y = (y_1, \dots, y_n)$ с минимальной ошибкой, где мерой ошибки выбирается среднеквадратичное отклонении, иными словами необходимо найти такую функцию $\hat{f}$, которая удовлетворяет следующему условию:

\begin{equation}\label{eq:noisesolve}
\hat{f} = min_{f^*} || f^* - f ||_2
\end{equation}

где $\hat{f} = \hat{f}(y)$. Должно быть понятно, что на практике функция $f$ неизвестна, так что используется оценка среднеквадратической ошибки.

```{r snr, echo=FALSE, message=FALSE, fig.cap='Пример чистого и зашумленного сигнала. Отношение сигнал шум -- 20 db', }
t <- seq(0, pi, 0.01)
clean <- 30 *sin(t)
noised <- 30 * sin(t) + rnorm(length(t))
visual.multiplot(qplot(t, clean, geom='line', ylab='x(t)'), qplot(t, noised, geom='line', ylab='x(t)'), cols=2)
```

Кроме того выражение \ref{eq:noiseq} не является общим, так как возможны другие (не аддитивные) отношения между исходным сигналом и стохастическим. Тем не менее \ref{eq:noiseq} представляет собой хорошую модель для многих практических приложений.

Не нарушая общности предположим, что $t_i$ лежат в единичном отрезке $[0, 1]$. Более того, для простоты, предположим что отсчеты временного ряда $y(t)$ равномерно распределены на этом отрезке, а так же размер выборки $n = 2^T$, где $T in \mathcal{N}$. Этм предположения позволяют произвести как прямое так и обратное дискретное вейвлет преобразование используя алгоритм Маллата. Далее приведем два метода для получения (приблизительного) решения уравнения \ref{eq:noisesolve}.

## Пороговый вейвлет метод
Ортогональность дискретного вейвлет преобразования (в предположении что используются ортогональные вейвлеты с периодическими граничными условиями) приводит к тому, что белый шум при преобразовании переходит в белый шум. Тем самым если положим $d_{jk}$ (где $j$ обозначает уровень декомпозиции, а $k$ показывает индекс коэффициента на данном уровне) коэффициенты вейвлет преобразования значений $y_i$ из уравнения \ref{eq:noiseq} то в вейвлет пространстве оно перепишется как:

\begin{equation}\label{eq:wavenoise}
    d_{jk} = \omega_{jk} + \sigma \tilde{\varepsilon}_{jk}
\end{equation}

где $\omega_{jk}$ представляют собой ``чистые вейвлет коэффициенты'' (преобразования исходной функции $f(t_i)$), а $\tilde{\varepsilon}_{jk}$ являются независимыми одинаково распределенными величинами с законом распределения $\mathcal{N}(0,~1)$. Тем самым вейвлет коэффициенты наблюдаемого сигнала могут рассматриваться как зашумленная версия вейвлет коэффициентов исходного сигнала. На первый взгляд, мы не получили никакого преимущества над оригинально постановкой задачи \ref{eq:noiseq}, но на самом деле это не так. Так как анализ сигнала в вейвлет пространстве имеет несколько преимуществ.

Коэффициенты вейвлет преобразования обычно являются разреженными. Это значит, что большая часть из вейвлет коэффициентов чистого сигнала достаточно близка к нулю (КАК ПОКАЗАНО НА ПРИМЕРЕ РИСУНОК). Таким образом, мы можем переформулировать исходную задачу восстановления исходной функции $f$, как задачу восстановления вейвлет коэффициентов функции $f$, которые относительно ``устойчивее'' чем белый гауссовый шум. Тем самым, коэффициенты с достаточно малыми значениями, могут рассматриваться как чистый шум и должны быть установлены в ноль. Подход в котором коэффициент сравнивается с пороговым значением для определения является ли это значение желаемым или нет, называется __пороговым вейвлет методом__.

Этот метод зачастую применяется лишь к тем вейвлет коэффициентам, которые отвечают за детализацию ($d_{jk}$), исключая из рассмотрения коэффициенты апрокимации ($c_{jk}$). Так как в последних содержится важная ``низкочастотная'' информация, которая обычно содержит важные составляющие сигнала, и на практике менее подвержена влиянию шума. Подход выделяет значимые коэффициенты устанавливая в 0 значения тех коэффициентов, чья величина по модулю ниже некоторого наперед заданного порогового значения, которое мы будем обозначать $\lambda$. В общем случае, пороговое значение $\lambda$ представляет собой функцию от уровня детализации $j$ и позиции $k$ на этом уровне:

$$ \lambda = \lambda(j, k) $$

Но обычно, эта функция зависит лишь от уровня детализации, представляя собой функционал вида:

$$ \lambda = \lambda(j) $$

В последнем случае такой функционал называют __пороговым значением зависящим от уровня__.

Замена вейвлет коэффициентов основанная на принятом пороговом значении $\lambda$ может быть как ``жестким'' так и мягким соотвественно:

$$
\delta^H_{\lambda} (d_{jk}) = \begin{cases}
    0, ~ |d_{jk}| \leq \lambda \\
    d_{jk}, ~ |d_{jk}| > \lambda
\end{cases}
$$

$$
\delta^S_{\lambda} (d_{jk}) = \begin{cases}
    0, ~ |d_{jk}| \leq \lambda \\
    d_{jk} - \lambda, ~ d_{jk} > \lambda \\
    d_{jk} + \lambda, ~ d_{jk} < -\lambda \\
\end{cases}
$$

где, как было замечено ранее, $\lambda$ может быть функцией от переменных $j$ и $k$. Жесткое правило обычно и называют пороговым вейвлет методом, вто время как мягкое правило обычно называют вейвлет сжатием, так как оно сжимает коэффициенты с высоким уровнем энергии к нулю. Примеры преобразований представлены на Рисунке \ref{fig:hardsoft}

```{r hardsoft, echo=FALSE, message=TRUE, fig.cap="Примеры мягкой и жесткой замены вейвлет коэффициентов"}
h <- function(x, lambda) {
  t <- c(x)
  t[abs(t) <= lambda] <- 0
  t
}

s <- function(x, lambda) {
  t <- c(x)
  t[abs(x) <= lambda] <- 0
  t[x < -lambda] <- t[x < -lambda] + lambda
  t[x > lambda] <- t[x > lambda] - lambda
  t
}

t <- seq(-10, 10, 0.01)
r.s <- s(t, 2)
r.h <- h(t, 2)

visual.multiplot(qplot(t, r.h, geom='line', ylab=expression(delta[h])), qplot(t, r.s, geom='line', ylab=expression(delta[s])), cols=2)

```

Большинство алгоритмов использующих порогвый вейвлет метод пытаются оценить оптимальное значение $\lambda$. Хотя первый шаг этих алгоритмов обычно включает в семя оценку уровня шума $\sigma$. Простое предположение, что уровень $\sigma$ пропорционален дисперсии коэффициентов, на практике является не лучшей оценкой, если только функция $f$ не представляет собой плоскость. Широко распространенной оценкой уровня шума $\sigma$ является оценка, предложенная Донобо и Йоханстноном[@donobo2]. Она основывается на последнем уровне детализации вейвлет разложения, следуя медианному значению абсолютного отклонения:

\begin{equation}
    \hat{\sigma} = \frac{m(|d_{J-1, k}|)_{k = 0}^{2^{J-1}-1}}{0.6745}
\end{equation}

где число в знаменателе является множителем мастшаба, который вообще говоря зависит от распределения коэффициентов $d_{jk}$ и равен $0.6745$ в случае нормально распределенных данных. В данной работе мы предполагаем, что уровень шума $\sigma$ известен, и для простоты положим его равным 1.

Далее будут расммотрены два метода, которые используют пороговый подход. Эти методы не предполагают никаких особых условий налагаемых на исходную функцию $f$ и подходят для оценки относительно общих функций.


## SureShrink
Донобо и Йоханстнон в своей работе [@donobo3] предложили схему, которая применяет $\lambda_j$ -- пороговое значение зависящее от уровня к вейвлет коэффициентам $d_{jk}$. Их схема основывается на небайесовском риске Штейна[@sure], который определяется следующим образом.

Пусть величины $X_1, \dots, X_s$ независимы и одинаково распределены с нормальным законом $\mathcal{N} (\mu_i, ~ 1), ~ (i = 1, \dots, s)$. Задача состоит в том, чтобы оценить вектор средних $\mu = (\mu_1, \dots \mu_s)$ из известной выборки $\boldsymbol{x} = (X_1, \dots, X_s)$ с мимимальным риском. Иными словами необходимо найти оценку, которая удовлетворяет

\begin{equation}
    \hat{\mu} = min_{\hat{mu}} || \mu - \hat{\mu} ||_2
\end{equation}

Точное значение риска на практике не известно, так как истинное значение $\mu$ неизвестно. Для того чтобы получить оценку риска без необходимости знать истинное значение $\mu$ Штейн показывает, что для каждой оценки $\mu$, которая может быть записана как $\hat{\mu} = \boldsymbol{x} + g(\boldsymbol{x})$, где $g : \mathbb{R}^s \rightarrow \mathbb{R}^s$ и $g$ слабо дифференцируема, тогда риск может быть оценен как:

\begin{equation}\label{eq:sure}
    SURE(\hat{\mu}) = E_{\mu} || \mu - \hat{\mu}||_2^2 = s + E_\mu[||g(\boldsymbol{x})||_2^2 + 2\nabla * g(\boldsymbol{x})]
\end{equation}

где $\nabla g(t)$ -- дивергенция $g$:

\begin{equation}
    \nabla \dot g(\boldsymbol{x}) = \sum_{i=1}^{s} \frac{\partial g_i}{\partial x_i}
\end{equation}

Положим $\omega_{jk}$ как $\boldsymbol{x}$. Используя выражение для мягкого порогового правила $\delta^S_{\lambda}$, можно заметить, что $\delta^S_{\lambda} (\boldsymbol{x}) = x + g(\boldsymbol{x})$, где:

\begin{equation}
    ||g(\boldsymbol{x})||_2^2 = \sum_{i = 1}^s [min(|X_i|, \lambda)]^2
\end{equation}

\begin{equation}
    \dot g(\boldsymbol{x}) = \sum_{i=1}^{s} 1_{[-\lambda, \lambda]}(X_i)
\end{equation}

($1_A(x)$ -- понимается в смысле индикаторной функции на множестве $A$). Теперь возможно получить явное выражение для $SURE$:

\begin{equation}\label{eq:sureex}
    SURE(\lambda, \boldsymbol{x}) = s + \sum_{i = 1}^s [min(|X_i|, \lambda)]^2 - 2 \cdot \# \{i: |X_i| < \lambda\}
\end{equation}

(Здесь под $\#$ обозначим можность множеста). Явным преимуществом \ref{eq:sureex} является тот факт, что неизвестный параметр $\mu$ не появляется. Понятно, что пороговое значение $\lambda$ из \ref{eq:sureex} должно быть выбрано таким образом, что значение $SURE$ минимально:

\begin{equation}\label{eq:suremin}
    \lambda^* = argmin_{0 \leq \lambda \leq \sqrt{2\log s}} SURE(\lambda; \boldsymbol{x})
\end{equation}

```{r lambdau, echo=FALSE, message=FALSE, fig.cap="Универсальное пороговое значение расчитанноне по выборке из N(0, 1) размером 1024", dev='png'}
t <- rnorm(1024)
qplot(1:1024, abs(t), geom='area', xlab='t', ylab='Амплитуда') + geom_hline(aes(yintercept=sqrt(2*log2(1024))), color='red')
```

Передт тем как продолжить, следует заметить, что при поиске оптимального порогового значения $\lambda$ в \ref{eq:suremin} не принимаются во внимание значения, которые больше чем $\lambda_U = \sqrt{2\log s}$ (которое назовем __универсальной границей__). Причина этого состоит в следующем. Следуя теореме доказанной в [@donobo2], если $X_1, \dots, X_s$ независимы и одинаково распределены с нормальным законом распределения $N(0, ~ 1)$, тогда:

\begin{equation}\label{eq:sureprob}
    P\{max_{1 \leq i \leq s} |X_i| > \sqrt{2\log s} ~ \frac{1}{\sqrt{\pi \log s }} \over{s \rightarrow \infty}{\rightarrow}\}
\end{equation}

Как показано на Рисунке \ref{fig:lambdau}

Верхняя граница по $|X_i|$ в \ref{eq:sureprob} может служить универсальным пороговым значением, так как коэффициенты с значениями меньше чем $\lambda_U$ с большой вероятностью являются шумом. Как было замечено ранее, вейвлет преобразование незашумленных объектов заполнено большим количеством нулевых коэффициентов (то есть разрежено). После зашумления эти коэффициенты становятся ненулевыми, и восстановленный сигнал приобретает нежелательные визуальные артефакты. Хотя ограничение $\lambda$ граничным значением $\sqrt{2 \log s}$, с большой вероятностью гарантирует, что каждое значение в вейвлет преобразовании наблюдаемого сигнала будет оценено как ноль, если значение вейвлет коэффициента истинного сигнала равно нулю. Следовательно $\lambda$ значения, которые больше чем установленное универсальное пороговое значение $\lambda_U$, не должны приниматься во внимание при решении оптимизационной задачи \ref{eq:suremin}.

Принятие универсального порогового значения $\lambda_U = \sqrt{ 2 \log s}$ в качестве порогового значения $\lambda$ на всех уровнях приводит к достаточно хорошим результатам и при этом не влечет никаких вычислительных сложностей на практике. Этот метод так же был предложен Донобо и Йоханстоном в [@donobo2] и известен как __VisuShrink__. Однако, этот метод зачастую слишком общиий и не адаптируется под конкретные исходные данные, кроме того как было замечено в [@donobo3], $\lambda^*$, являющееся решением \ref{eq:suremin} в этом смысле подходит для решения прикладных задач заметно больше чем универсальное пороговое значение $\lambda_U$.

Решение оптимизационной задачи \ref{eq:suremin} может быть получено без каких-либо дополнительных преобразований или вычислительных трудностей. Если выборка $X_i$ упорядоченна в смысле возрастания абсолютных значений $|X_i|$ (на практике это может быть реализовано со сложностью $O(s \log s)$ с использованием сортировки слиянием), тогда из \ref{eq:sureex} следует, что на интервалах $\lambda$ которые лежат между двумя значениями $|X_i|$ оценка $SURE$ строго возрастает. Тогда, искомое значение $\lambda^*$ должно быть равно одному из значений $|X_i|$, которые в свою очередь являются детализационными коэффициентами $d_{jk}$ вейвлет преобразования наблюдаемого сигнала, для некоторого фиксированного значения уровня детализации $j$. Слежовательно оценка $SURE$ должна быть посчитана для неболее чем $s$ коэффициентов, требуя дополнительной сложности оцениваемой как $O(s)$. Тем самым общая вычислительная сложность составляет

$$  O(s \log s) + O(s) = O(s \log s) $$

Пример решения \ref{eq:suremin} приведен на РИСУНКЕ. В этом примере, размерность $\mu$ составляет $s = 128$. Сам же вектор состоит из 16 последовательных четверок, после которых стоят одни нули. И аддитивный белый шум Гаусса имеет дисперсию равную 1. Используя $SURE$ и находя минимум в задаче \ref{eq:sureex} получаем требуемое пороговое значение.

