Существует довольно много приложений, где подавление шума играет важнуую роль. Хорошим примером могут быть анализ изображений и сигналов в медицинских ислледованиях, радиоастрономии и датамайнинге и прочих. В каждой конкретной области имеются свои специальные требования. Например устранение шума в медицине требует особой точности и аккуратности, так как шумоподавление использующее сглаживание зашумленного сигнала (то есть используя LP-фильтр[^filters]) может привести к потере точных и важных деталей. Как это показано на [@fig2]

Следсвием острой необходимости в качественных методах устранения шума является достаточно большое количество подходов к решению задачи фильтрации, которые в свою очередь могут быть разделены на две категории:

* фильтрация происходит в пространстве исходного сигнала (то есть время или пространство)
* фильтрация происходит в пространстве примененного преобразования (например преобразования Фурье)

Активное развитие теории вейвлет-анализа и вейвлет-преобразований, происходящая в последние два десятилетия революционным образом изменила подходы применяемые в обработке сигналов и изображений, особенно в области шумоподавления. Начиная с 1990-ых, в этой области преобладающими методами были __вейвлет сжатие(shrinkage)__ и __пороговый вейвлет метод__ (подробнее о них будет рассказано позже)[@donobo1, @donobo2, @donobo3, @donobo4, @donobo5].
Обзор этих подходов дан в работе [@six].

В этой главе представлены и реализованы два метода шумоподавления. Первый -- __SureShrink__, представлен в работе Донобо и Йохонстона [@donobo3]. Второй был представлен Каи и Сильверманом в [@silver7], и называется __NeighBlock__.

## Каскадный алгоритм Маллата

## Постановка задачи

Распространенная формулировка задачи фильтрации звучит следующим образом. Пусть имеется выборка зашумленных значений некоторой функции $f$ размера $n$:

\begin{equation}\label{eq:noiseq}
    y_i = f(t_i) + \sigma \varepsilon_i, ~ i = 1 \dots n
\end{equation}

где $\varepsilon_i$ независимые одинаково распределенные с законом $\mathcal{N}(0,~1)$ случайные величины, а уровень шума $\sigma$ может быть неизвестен. Пример такой выборки приведен на рисунке [@fig6]. Основной задачей является выделение значений исходной функции $f$ из зашумленной выборки $y = (y_1, \dots, y_n)$ с минимальной ошибкой, где мерой ошибки выбирается среднеквадратичное отклонении, иными словами необходимо найти такую функцию $\hat{f}$, которая удовлетворяет следующему условию:

\begin{equation}\label{eq:noisesolve}
\hat{f} = min_{f^*} || f^* - f ||_2
\end{equation}

где $\hat{f} = \hat{f}(y)$. Должно быть понятно, что на практике функция $f$ неизвестна, так что используется оценка среднеквадратической ошибки.

Кроме того выражение noiseq не является общим, так как возможны другие (не аддитивные) отношения между исходным сигналом и стохастическим. Тем не менее \ref{eq:noiseq} представляет собой хорошую модель для многих практических приложений.

Не нарушая общности предположим, что $t_i$ лежат в единичном отрезке $[0, 1]$. Более того, для простоты, предположим что отсчеты временного ряда $y(t)$ равномерно распределены на этом отрезке, а так же размер выборки $n = 2^T$, где $T in \mathcal{N}$. Этм предположения позволяют произвести как прямое так и обратное дискретное вейвлет преобразование используя алгоритм Маллата. Далее приведем два метода для получения (приблизительного) решения уравнения \ref{eq:noisesolve}.

## Пороговый вейвлет метод
Ортогональность дискретного вейвлет преобразования (в предположении что используются ортогональные вейвлеты с периодическими граничными условиями) приводит к тому, что белый шум при преобразовании переходит в белый шум. Тем самым если положим $d_{jk}$ (где $j$ обозначает уровень декомпозиции, а $k$ показывает индекс коэффициента на данном уровне) коэффициенты вейвлет преобразования значений $y_i$ из уравнения \ref{eq:noiseq} то в вейвлет пространстве оно перепишется как:

\begin{equation}\label{eq:wavenoise}
    d_{jk} = \omega_{jk} + \sigma \tilde{\varepsilon}_{jk}
\end{equation}

где $\omega_{jk}$ представляют собой ``чистые вейвлет коэффициенты'' (преобразования исходной функции $f(t_i)$), а $\tilde{\varepsilon}_{jk}$ являются независимыми одинаково распределенными величинами с законом распределения $\mathcal{N}(0,~1)$. Тем самым вейвлет коэффициенты наблюдаемого сигнала могут рассматриваться как зашумленная версия вейвлет коэффициентов исходного сигнала. На первый взгляд, мы не получили никакого преимущества над оригинально постановкой задачи \ref{eq:noiseq}, но на самом деле это не так. Так как анализ сигнала в вейвлет пространстве имеет несколько преимуществ.

Коэффициенты вейвлет преобразования обычно являются разреженными. Это значит, что большая часть из вейвлет коэффициентов чистого сигнала достаточно близка к нулю (КАК ПОКАЗАНО НА ПРИМЕРЕ РИСУНОК). Таким образом, мы можем переформулировать исходную задачу восстановления исходной функции $f$, как задачу восстановления вейвлет коэффициентов функции $f$, которые относительно ``устойчивее'' чем белый гауссовый шум. Тем самым, коэффициенты с достаточно малыми значениями, могут рассматриваться как чистый шум и должны быть установлены в ноль. Подход в котором коэффициент сравнивается с пороговым значением для определения является ли это значение желаемым или нет, называется __пороговым вейвлет методом__.

Этот метод зачастую применяется лишь к тем вейвлет коэффициентам, которые отвечают за детализацию ($d_{jk}$), исключая из рассмотрения коэффициенты апрокимации ($c_{jk}$). Так как в последних содержится важная ``низкочастотная'' информация, которая обычно содержит важные составляющие сигнала, и на практике менее подвержена влиянию шума. Подход выделяет значимые коэффициенты устанавливая в 0 значения тех коэффициентов, чья величина по модулю ниже некоторого наперед заданного порогового значения, которое мы будем обозначать $\lambda$. В общем случае, пороговое значение $\lambda$ представляет собой функцию от уровня детализации $j$ и позиции $k$ на этом уровне:

$$ \lambda = \lambda(j, k) $$

Но обычно, эта функция зависит лишь от уровня детализации, представляя собой функционал вида:

$$ \lambda = \lambda(j) $$

В последнем случае такой функционал называют __пороговым значением зависящим от уровня__.

Замена вейвлет коэффициентов основанная на принятом пороговом значении $\lambda$ может быть как ``жестким'' так и мягким соотвественно:

$$
\delta^H_{\lambda} (d_{jk}) = \begin{cases}
    0, ~ |d_{jk}| \leq \lambda \\
    d_{jk}, ~ |d_{jk}| > \lambda
\end{cases}
$$

$$
\delta^S_{\lambda} (d_{jk}) = \begin{cases}
    0, ~ |d_{jk}| \leq \lambda \\
    d_{jk} - \lambda, ~ d_{jk} > \lambda \\
    d_{jk} + \lambda, ~ d_{jk} < -\lambda \\
\end{cases}
$$

где, как было замечено ранее, $\lambda$ может быть функцией от переменных $j$ и $k$. Жесткое правило обычно и называют пороговым вейвлет методом, вто время как мягкое правило обычно называют вейвлет сжатием, так как оно сжимает коэффициенты с высоким уровнем энергии к нулю. ПРИМЕРЫ ПРЕОБРАЗОВАНИЙ ПРЕДСТАВЛЕНЫ НА КАРТИНКЕ.

Большинство алгоритмов использующих порогвый вейвлет метод пытаются оценить оптимальное значение $\lambda$. Хотя первый шаг этих алгоритмов обычно включает в семя оценку уровня шума $\sigma$. Простое предположение, что уровень $\sigma$ пропорционален дисперсии коэффициентов, на практике является не лучшей оценкой, если только функция $f$ не представляет собой плоскость. Широко распространенной оценкой уровня шума $\sigma$ является оценка, предложенная Донобо и Йоханстноном[@donobo2]. Она основывается на последнем уровне детализации вейвлет разложения, следуя медианному значению абсолютного отклонения:

\begin{equation}
    \hat{\sigma} = \frac{m(|d_{J-1, k}|)_{k = 0}^{2^{J-1}-1}}{0.6745}
\end{equation}

где число в знаменателе является множителем мастшаба, который вообще говоря зависит от распределения коэффициентов $d_{jk}$ и равен $0.6745$ в случае нормально распределенных данных. В данной работе мы предполагаем, что уровень шума $\sigma$ известен, и для простоты положим его равным 1.

Далее будут расммотрены два метода, которые используют пороговый подход. Эти методы не предполагают никаких особых условий налагаемых на исходную функцию $f$ и подходят для оценки относительно общих функций.


## SureShrink
Донобо и Йоханстнон в своей работе [@donobo3] предложили схему, которая применяет $\lambda_j$ -- пороговое значение зависящее от уровня к вейвлет коэффициентам $d_{jk}$. Их схема основывается на небайесовском риске Штейна[@sure], который определяется следующим образом.

Пусть величины $X_1, \dots, X_s$ независимы и одинаково распределены с нормальным законом $\mathcal{N} (\mu_i, ~ 1), ~ (i = 1, \dots, s) $. Задача состоит в том, чтобы оценить вектор средних $\mu = (\mu_1, \dots \mu_s)$ из известной выборки $\boldsymbol{x} = (X_1, \dots, X_s)$ с мимимальным риском. Иными словами необходимо найти оценку, которая удовлетворяет

\begin{equeation}
    \hat{\mu} = min_{\hat{mu}} || \mu - \hat{\mu} ||_2
\end{equation}

Точное значение риска на практике не известно, так как истинное значение $\mu$ неизвестно. Для того чтобы получить оценку риска без необходимости знать истинное значение $\mu$ Штейн показывает, что для каждой оценки $\mu$, которая может быть записана как $\hat{\mu} = \boldsymbol{x} + g(\boldsymbol{x})$, где $g : \mathbb{R}^s \rightarrow \mathbb{R}^s$ и $g$ слабо дифференцируема, тогда риск может быть оценен как:

\begin{equation}\label{eq:sure}
    SURE(\hat{\mu}) = E_{\mu} || \mu - \hat{\mu}||_2^2 = s + E_\mu[||g(\boldsymbol{x})||_2^2 + 2\nabla * g(\boldsymbol{x})]
\end{equation}

где $\nabla \dot g(\dot)$ -- дивергенция $g$:

\begin{equation}
    \nabla \dot g(\boldsymbol{x}) = \sum_{i=1}^{s} \frac{\partial g_i}{\partial x_i}
\end{equation}

Положим $\omega_{jk}$ как $\boldsymbol{x}$. Используя выражение для мягкого порогового правила $\delta^S_{\lambda}$, можно заметить, что $\delta^S_{\lambda} (\boldsymbol{x}) = x + g(\boldsymbol{x})$, где:

\begin{equation}
    ||g(\boldsymbol{x})||_2^2 = \sum_{i = 1}^s [min(|X_i|, \lambda)]^2
\end{equation}

\begin{equation}
    \dot g(\boldsymbol{x}) = \sum_{i=1}^{s} 1_{[-\lambda, \lambda]}(X_i)
\end{equation}

($1_A(x)$ -- понимается в смысле индикаторной функции на множестве $A$). Теперь возможно получить явное выражение для $SURE$:

\begin{equation}\label{eq:sureex}
    SURE(\lambda, \boldsymbol{x}) = s + \sum_{i = 1}^s [min(|X_i|, \lambda)]^2 - 2 \cdot # \{i: |X_i| < \lambda\}
\end{equation}

(Здесь под $#$ обозначим можность множеста). Явным преимуществом \ref{eq:sureex} является тот факт, что неизвестный параметр $\mu$ не появляется. Понятно, что пороговое значение $\lambda$ из \ref{eq:sureex} должно быть выбрано таким образом, что значение $SURE$ минимально:

\begin{equation}\label{eq:suremin}
    \lambda^* = argmin_{0 \leq \lambda \leq \sqrt{2\log s}} SURE(\lambda; \boldsymbol{x})
\end{equation}

```{r}
"EXAMPLE PICTURE"
```

Передт тем как продолжить, следует заметить, что при поиске оптимального порогового значения $\lambda$ в \ref{eq:suremin} не принимаются во внимание значения, которые больше чем $\lambda_U = \sqrt{2\log s}$ (которое назовем __универсальной границей__). Причина этого состоит в следующем. Следуя теореме доказанной в [@donobo2], если $X_1, \dots, X_s$ независимы и одинаково распределены с нормальным законом распределения $\N(0, ~ 1)$, тогда:

\begin{equation}
    P\{max_{1 \leq i \leq s} |X_i| > \sqrt{2\log s} ~ \frac{1}{\sqrt{\pi \log s }} \over{s \rightarrow \infty}{\rightarrow}\}
\end{equation}

Как показано на рисунке #EXAMPLE PICTURE

Верхняя граница

## NeighBlock
