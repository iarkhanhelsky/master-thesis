
# Вейвлет анализ в задачах фильтрации
Достаточно известный факт, что в на практике редко приходитя работать с данными без шума, который, однако, может быть незначительным (в смысле отношения сигнал/шум) в некоторых условиях. Тем не менее, достаточно часто шум повреждает сигнал в значительной мере и должен быть устранен из данных для обеспечения возможности проведения дальнейшего анализа. Процесс устранения шума из сигнала в общем случае называется __фильтрацией__ (или __шумоподавлением__). Примеры чистого и зашумленного сигналов привелены на рисунке \ref{fig:noiseex}. Можно заметить, что шум добавляет высокочастотные компоненты в изначально достаточно гладки сигнал. Это является характеристической особенностью шума.


```{r noiseex, echo=FALSE, fig.cap="Примеры зашумленного и оригинального сигнала", out.extra=''}
require(reshape2)
require(ggplot2)
t <- seq(0, 3*pi, 0.01)
data <- data.frame(
  noised = 3*sin(t) + rnorm(length(t)),
  clean = 3*sin(t),
  t = t)
data.long <- melt(data, id='t', value.name='x')

ggplot(data=data.long, aes(x=t, y=x, colour=variable)) +
  theme(legend.position="none") +
  geom_line()
```

Не смотря на то, что понятие ``фильтрация'' достаточно общеее, оно обычно относится к восстановлению сигнала, который был смешан с аддитивным белым Гауссовым шумом, нежели с какими-либо другими видами шумов (такими как не-аддитивный шум, шумы Пуассона, Лапласса и прочие). В этой главе мы сосредоточимся на случае, когда сигнал подвержен воздействию белого шума.

Оптимизационный критерий, следуя которому измеряется эффективность алгоритмов шумоподавления, обычно выбирается в смысле среднеквадратической ошибки, между оригинальным сигналом (если он существует) и его восстановленной версией. Этот общий критерий выбирается из соображений простоты вычисления. Более того, обычно приводит к аналитически легко вычисляемым выражениям. Однако, он может быть недопустимым в некоторых задачах, которых точность восстановления сигнала является определяющей, хотя само понятие точности является спорным, особенно в отсуствии оригинального сигнала.

Существует довольно много приложений, где подавление шума играет важную роль. Хорошим примером могут быть анализ изображений и сигналов в медицинских ислледованиях, радиоастрономии, датамайнинге и прочих. В каждой конкретной области имеются свои специальные требования. Например устранение шума в медицине требует особой точности и аккуратности, так как шумоподавление использующее сглаживание зашумленного сигнала (то есть используя НЧ-фильтр[^filters]) может привести к потере точных и важных деталей. Как это показано на \ref{img:brain}

[^filters]:
  Фильтр нижних частот (ФНЧ) — фильтр, эффективно пропускающий частотный спектр сигнала ниже некоторой частоты (частоты среза), и уменьшающий (подавляющий) частоты сигнала выше этой частоты.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{../src/res/mri.jpg}
  \center{а)}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{../src/res/mri_noise.jpg}
  \center{б)}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{../src/res/mri_denoise.jpg}
  \center{в)}
\endminipage
\caption{а) Исходное изображение МРТ мозга б) Исходное изображение МРТ мозга с наложенным шумом в) Результат шумоподавления с помощью фильтра Гаусса}\label{img:brain}
\end{figure}


Следсвием острой необходимости в качественных методах устранения шума является достаточно большое количество подходов к решению задачи фильтрации, которые в свою очередь могут быть разделены на две категории:

* фильтрация происходит в пространстве исходного сигнала (то есть время или пространство)
* фильтрация происходит в пространстве примененного преобразования (например преобразования Фурье)

Активное развитие теории вейвлет-анализа и вейвлет-преобразований, происходящая в последние два десятилетия революционным образом изменила подходы применяемые в обработке сигналов и изображений, особенно в области шумоподавления. Начиная с 1990-ых, в этой области преобладающими методами были __вейвлет сжатие(shrinkage)__ и __пороговый вейвлет метод__ (подробнее о них будет рассказано позже)[@donobo1][@donobo2][@donobo3][@donobo4][@donobo5].
Обзор этих подходов дан в работе [@six].

В этой главе представлены и реализованы два метода шумоподавления. Первый -- __SureShrink__, представлен в работе Донобо и Йохонстона [@donobo3]. Второй был представлен Каи и Сильверманом в [@silver7], и называется __NeighBlock__.


```{r decomp, echo=FALSE, message=FALSE, fig.cap="Пример разложения модельных данных с помощью алгоритма Маллата используя вейвлет Добеши четвертого порядка(в качестве данных использовались показания катировок продаж Box\\&Jenkins, 1976 год). а) Оригинальные данные. b) кэффициенты второго уровня апроксимации c) детализационные коэффициенты, j=1", out.extra=''}

d <- BJsales[1:128]

w <- dwt(d, filter='d4')

p1 <- ggplot() + geom_line(aes(1:length(d), d)) + xlab('t') + ylab('') + ggtitle('a)')
p2 <- ggplot() + geom_line(aes(1:length(w@V$V2), w@V$V2)) + xlab('k') + ylab(expression(c[jk])) + ggtitle('b)')
p3 <- ggplot() + geom_line(aes(1:length(w@W$W1), w@W$W1)) + xlab('k') + ylab(expression(d[jk])) + ggtitle('c)')

visual.multiplot(p2, p3, p1, layout=t(matrix(c(3, 3, 1, 2), nrow=2)), cols=2)

```

## Постановка задачи фильтрации сигнала

Распространенная формулировка задачи фильтрации звучит следующим образом. Пусть имеется выборка зашумленных значений некоторой функции $f$ размера $n$:

\begin{equation}\label{eq:noiseq}
    y_i = f(t_i) + \sigma \varepsilon_i, ~ i = 1 \dots n
\end{equation}

где $\varepsilon_i$ независимые одинаково распределенные с законом ${N}(0,~1)$ случайные величины, а уровень шума $\sigma$ может быть неизвестен. Пример такой выборки приведен на Рисунке \ref{fig:snr}. Основной задачей является выделение значений исходной функции $f$ из зашумленной выборки $y = (y_1, \dots, y_n)$ с минимальной ошибкой, где мерой ошибки выбирается среднеквадратичное отклонении, иными словами необходимо найти такую функцию $\hat{f}$, которая удовлетворяет следующему условию:

\begin{equation}\label{eq:noisesolve}
\hat{f} = \underset{f^*}{min} || f^* - f ||_2
\end{equation}

где $\hat{f} = \hat{f}(y)$. Должно быть понятно, что на практике функция $f$ неизвестна, так что используется оценка среднеквадратической ошибки.

```{r snr, echo=FALSE, message=FALSE, fig.cap='Пример чистого и зашумленного сигнала. Отношение сигнал шум -- 20 db', out.extra='' }
t <- seq(0, pi, 0.01)
clean <- 30 *sin(t)
noised <- 30 * sin(t) + rnorm(length(t))
visual.multiplot(qplot(t, clean, geom='line', ylab='x(t)'), qplot(t, noised, geom='line', ylab='x(t)'), cols=2)
```

Кроме того выражение \ref{eq:noiseq} не является общим, так как возможны другие (не аддитивные) отношения между исходным сигналом и стохастическим. Тем не менее \ref{eq:noiseq} представляет собой хорошую модель для многих практических приложений.

Не нарушая общности предположим, что $t_i$ лежат в единичном отрезке $[0, 1]$. Более того, для простоты, предположим что отсчеты временного ряда $y(t)$ равномерно распределены на этом отрезке, а так же размер выборки $n = 2^T$, где $T \in \mathbb{N}$. Этм предположения позволяют произвести как прямое так и обратное дискретное вейвлет преобразование используя алгоритм Маллата. Далее приведем два метода для получения (приблизительного) решения уравнения \ref{eq:noisesolve}.

## Пороговый вейвлет метод
Ортогональность дискретного вейвлет преобразования (в предположении что используются ортогональные вейвлеты с периодическими граничными условиями) приводит к тому, что белый шум при преобразовании переходит в белый шум. Тем самым если положим $d_{jk}$ (где $j$ обозначает уровень декомпозиции, а $k$ показывает индекс коэффициента на данном уровне) коэффициенты вейвлет преобразования значений $y_i$ из уравнения \ref{eq:noiseq} то в вейвлет пространстве оно перепишется как:

\begin{equation}\label{eq:wavenoise}
    d_{jk} = \omega_{jk} + \sigma \tilde{\varepsilon}_{jk}
\end{equation}

где $\omega_{jk}$ представляют собой ``чистые вейвлет коэффициенты'' (преобразования исходной функции $f(t_i)$), а $\tilde{\varepsilon}_{jk}$ являются независимыми одинаково распределенными величинами с законом распределения $\mathcal{N}(0,~1)$. Тем самым вейвлет коэффициенты наблюдаемого сигнала могут рассматриваться как зашумленная версия вейвлет коэффициентов исходного сигнала. На первый взгляд, мы не получили никакого преимущества над оригинально постановкой задачи \ref{eq:noiseq}, но на самом деле это не так. Так как анализ сигнала в вейвлет пространстве имеет несколько преимуществ.

Коэффициенты вейвлет преобразования обычно являются разреженными. Это значит, что большая часть из вейвлет коэффициентов чистого сигнала достаточно близка к нулю (видно из Рисунка \ref{fig:decomp}). Таким образом, мы можем переформулировать исходную задачу восстановления исходной функции $f$, как задачу восстановления вейвлет коэффициентов функции $f$, которые относительно ``устойчивее'' чем белый гауссовый шум. Тем самым, коэффициенты с достаточно малыми значениями, могут рассматриваться как чистый шум и должны быть установлены в ноль. Подход в котором коэффициент сравнивается с пороговым значением для определения является ли это значение желаемым или нет, называется __пороговым вейвлет методом__.

Этот метод зачастую применяется лишь к тем вейвлет коэффициентам, которые отвечают за детализацию ($d_{jk}$), исключая из рассмотрения коэффициенты апрокимации ($c_{jk}$). Так как в последних содержится важная ``низкочастотная'' информация, которая обычно содержит важные составляющие сигнала, и на практике менее подвержена влиянию шума. Подход выделяет значимые коэффициенты устанавливая в 0 значения тех коэффициентов, чья величина по модулю ниже некоторого наперед заданного порогового значения, которое мы будем обозначать $\lambda$. В общем случае, пороговое значение $\lambda$ представляет собой функцию от уровня детализации $j$ и позиции $k$ на этом уровне:

$$ \lambda = \lambda(j, k) $$

Но обычно, эта функция зависит лишь от уровня детализации, представляя собой функционал вида:

$$ \lambda = \lambda(j) $$

В последнем случае такой функционал называют __пороговым значением зависящим от уровня__.

Замена вейвлет коэффициентов основанная на принятом пороговом значении $\lambda$ может быть как ``жестким'' так и мягким соотвественно:

$$
\delta^H_{\lambda} (d_{jk}) = \begin{cases}
    0, ~ |d_{jk}| \leq \lambda \\
    d_{jk}, ~ |d_{jk}| > \lambda
\end{cases}
$$

$$
\delta^S_{\lambda} (d_{jk}) = \begin{cases}
    0, ~ |d_{jk}| \leq \lambda \\
    d_{jk} - \lambda, ~ d_{jk} > \lambda \\
    d_{jk} + \lambda, ~ d_{jk} < -\lambda \\
\end{cases}
$$

где, как было замечено ранее, $\lambda$ может быть функцией от переменных $j$ и $k$. Жесткое правило обычно и называют пороговым вейвлет методом, в то время как мягкое правило обычно называют вейвлет сжатием, так как оно сжимает коэффициенты с высоким уровнем энергии к нулю. Примеры преобразований представлены на Рисунке \ref{fig:hardsoft}

```{r hardsoft, echo=FALSE, message=TRUE, fig.cap="Иллюстрация жесткой и мягкой замены вейвлет коэффициентов соотвественно $(\\lambda = 2)$", out.extra=''}
t <- seq(-10, 10, 0.01)
r.s <- thresholding.soft(t, 2)
r.h <- thresholding.hard(t, 2)

visual.multiplot(qplot(t, r.h, geom='line', ylab=expression(delta[h])), qplot(t, r.s, geom='line', ylab=expression(delta[s])), cols=2)

```

Большинство алгоритмов использующих порогвый вейвлет метод пытаются оценить оптимальное значение $\lambda$. Хотя первый шаг этих алгоритмов обычно включает в семя оценку уровня шума $\sigma$. Простое предположение, что уровень $\sigma$ пропорционален дисперсии коэффициентов, на практике является не лучшей оценкой, если только функция $f$ не представляет собой плоскость. Широко распространенной оценкой уровня шума $\sigma$ является оценка, предложенная Донобо и Йоханстноном[@donobo2]. Она основывается на последнем уровне детализации вейвлет разложения, следуя медианному значению абсолютного отклонения:

\begin{equation}
    \hat{\sigma} = \frac{m(|d_{J-1, k}|)_{k = 0}^{2^{J-1}-1}}{0.6745}
\end{equation}

где число в знаменателе является множителем мастшаба, который вообще говоря зависит от распределения коэффициентов $d_{jk}$ и равен $0.6745$ в случае нормально распределенных данных. В данной работе мы предполагаем, что уровень шума $\sigma$ известен, и для простоты положим его равным 1.

Далее будут расммотрены два метода, которые используют пороговый подход. Эти методы не предполагают никаких особых условий налагаемых на исходную функцию $f$ и подходят для оценки относительно общих функций.


## SureShrink
Донобо и Йоханстнон в своей работе [@donobo3] предложили схему, которая применяет $\lambda_j$ -- пороговое значение зависящее от уровня к вейвлет коэффициентам $d_{jk}$. Их схема основывается на небайесовском риске Штейна[@stein], который определяется следующим образом.

Пусть величины $X_1, \dots, X_s$ независимы и одинаково распределены с нормальным законом $\mathcal{N} (\mu_i, ~ 1), ~ (i = 1, \dots, s)$. Задача состоит в том, чтобы оценить вектор средних $\mu = (\mu_1, \dots \mu_s)$ из известной выборки $\mathbf{x} = (X_1, \dots, X_s)$ с мимимальным риском. Иными словами необходимо найти оценку, которая удовлетворяет

\begin{equation}
    \hat{\mu} = \underset{\hat{\mu}}{min} || \mu - \hat{\mu} ||_2
\end{equation}

Точное значение риска на практике не известно, так как истинное значение $\mu$ неизвестно. Для того чтобы получить оценку риска без необходимости знать истинное значение $\mu$ Штейн показывает, что для каждой оценки $\mu$, которая может быть записана как $\hat{\mu} = \mathbf{x} + g(\mathbf{x})$, где $g : \mathbb{R}^s \rightarrow \mathbb{R}^s$ и $g$ слабо дифференцируема, тогда риск может быть оценен как:

\begin{equation}\label{eq:sure}
    SURE(\hat{\mu}) = E_{\mu} || \mu - \hat{\mu}||_2^2 = s + E_\mu[||g(\mathbf{x})||_2^2 + 2\nabla * g(\mathbf{x})]
\end{equation}

где $\nabla g(t)$ -- дивергенция $g$:

\begin{equation}
    \nabla \dot g(\mathbf{x}) = \sum_{i=1}^{s} \frac{\partial g_i}{\partial x_i}
\end{equation}

Положим $\omega_{jk}$ как $\mathbf{x}$. Используя выражение для мягкого порогового правила $\delta^S_{\lambda}$, можно заметить, что $\delta^S_{\lambda} (\mathbf{x}) = x + g(\mathbf{x})$, где:

\begin{equation}
    ||g(\mathbf{x})||_2^2 = \sum_{i = 1}^s [min(|X_i|, \lambda)]^2
\end{equation}

\begin{equation}
    \dot g(\mathbf{x}) = \sum_{i=1}^{s} 1_{[-\lambda, \lambda]}(X_i)
\end{equation}

($1_A(x)$ -- понимается в смысле индикаторной функции на множестве $A$). Теперь возможно получить явное выражение для $SURE$:

\begin{equation}\label{eq:sureex}
    SURE(\lambda, \mathbf{x}) = s + \sum_{i = 1}^s [min(|X_i|, \lambda)]^2 - 2 \cdot \# \{i: |X_i| < \lambda\}
\end{equation}

(Здесь под $\#$ обозначим мощность множеста). Явным преимуществом \ref{eq:sureex} является тот факт, что неизвестный параметр $\mu$ не появляется. Понятно, что пороговое значение $\lambda$ из \ref{eq:sureex} должно быть выбрано таким образом, что значение $SURE$ минимально:

\begin{equation}\label{eq:suremin}
    \lambda^* = \underset{0 \leq \lambda \leq \sqrt{2\log s}}{argmin} SURE(\lambda; \mathsf{\mathbf{x})}
\end{equation}

```{r lambdau, echo=FALSE, message=FALSE, fig.cap="Универсальное пороговое значение расчитанноне по выборке из N(0, 1) размером 10000", dev='png', dpi=128, out.extra=''}
L <- 10000
t <- rnorm(L)
qplot(1:L, abs(t), geom='area', xlab='t', ylab='Амплитуда') + geom_hline(aes(yintercept=lambda.universal(L)), color='red')
```

Передт тем как продолжить, следует заметить, что при поиске оптимального порогового значения $\lambda$ в \ref{eq:suremin} не принимаются во внимание значения, которые больше чем $\lambda_U = \sqrt{2\log s}$ (которое назовем __универсальной границей__). Причина этого состоит в следующем. Следуя теореме доказанной в [@donobo2], если $X_1, \dots, X_s$ независимы и одинаково распределены с нормальным законом распределения $N(0, ~ 1)$, тогда:

\begin{equation}\label{eq:sureprob}
    P\{max_{1 \leq i \leq s} |X_i| > \sqrt{2\log s}\} \mathtt{\sim} \frac{1}{\sqrt{\pi \log s }} \underset{s \rightarrow \infty}{\rightarrow} 0
\end{equation}

Как показано на Рисунке \ref{fig:lambdau}

Верхняя граница по $|X_i|$ в \ref{eq:sureprob} может служить универсальным пороговым значением, так как коэффициенты с значениями меньше чем $\lambda_U$ с большой вероятностью являются шумом. Как было замечено ранее, вейвлет преобразование незашумленных объектов заполнено большим количеством нулевых коэффициентов (то есть разрежено). После зашумления эти коэффициенты становятся ненулевыми, и восстановленный сигнал приобретает нежелательные визуальные артефакты. Хотя ограничение $\lambda$ граничным значением $\sqrt{2 \log s}$, с большой вероятностью гарантирует, что каждое значение в вейвлет преобразовании наблюдаемого сигнала будет оценено как ноль, если значение вейвлет коэффициента истинного сигнала равно нулю. Следовательно $\lambda$ значения, которые больше чем установленное универсальное пороговое значение $\lambda_U$, не должны приниматься во внимание при решении оптимизационной задачи \ref{eq:suremin}.

Принятие универсального порогового значения $\lambda_U = \sqrt{ 2 \log s}$ в качестве порогового значения $\lambda$ на всех уровнях приводит к достаточно хорошим результатам и при этом не влечет никаких вычислительных сложностей на практике. Этот метод так же был предложен Донобо и Йоханстоном в [@donobo2] и известен как __VisuShrink__. Однако, этот метод зачастую слишком общиий и не адаптируется под конкретные исходные данные, кроме того как было замечено в [@donobo3], $\lambda^*$, являющееся решением \ref{eq:suremin} в этом смысле подходит для решения прикладных задач заметно больше чем универсальное пороговое значение $\lambda_U$.

Решение оптимизационной задачи \ref{eq:suremin} может быть получено без каких-либо дополнительных преобразований или вычислительных трудностей. Если выборка $X_i$ упорядоченна в смысле возрастания абсолютных значений $|X_i|$ (на практике это может быть реализовано со сложностью $O(s \log s)$ с использованием сортировки слиянием), тогда из \ref{eq:sureex} следует, что на интервалах $\lambda$ которые лежат между двумя значениями $|X_i|$ оценка $SURE$ строго возрастает. Тогда, искомое значение $\lambda^*$ должно быть равно одному из значений $|X_i|$, которые в свою очередь являются детализационными коэффициентами $d_{jk}$ вейвлет преобразования наблюдаемого сигнала, для некоторого фиксированного значения уровня детализации $j$. Слежовательно оценка $SURE$ должна быть посчитана для неболее чем $s$ коэффициентов, требуя дополнительной сложности оцениваемой как $O(s)$. Тем самым общая вычислительная сложность составляет

$$  O(s \log s) + O(s) = O(s \log s) $$

Пример решения \ref{eq:suremin} приведен на РИСУНКЕ. В этом примере, размерность $\mu$ составляет $s = 128$. Сам же вектор состоит из 16 последовательных четверок, после которых стоят одни нули. И аддитивный белый шум Гаусса имеет дисперсию равную 1. Используя $SURE$ и находя минимум в задаче \ref{eq:sureex} получаем требуемое пороговое значение.

Однако, как замечено в [@donobo3], выбор $\lambda^*$ следуя \ref{eq:suremin} не всегда является оптимальным. Это происходит из-за значительной разреженности вейвелет коэффициентов. В случае значительной разреженности, уровень шума влияющий на профиль SURE во многих отсчетах где сигнал близок к нулю становится более значительным, чем влияние ненулевых отсчетов. Тогда предлагается смешанная схема, которую называют __SureShrink__.

Следуя идеологии метода, пороговое значение в разреженном случае должно быть установлено равным универсяльному граничному значению $\lambda_U$, тогда с высокой вероятностью шумы будут обрезаны до нуля. Это желаемый результат, так как достаточно много значений сигнала __правильно__ усечены до нуля. Если вейвлет коэффициенты не достаточно разрежены, предполагается что SURE дает хорошую оценку потерь, и используется $\lambda^*$

Желая определить какое из значений нам необходимо использовать, мы вводим __меру разреженности__ вейвлет коэффициентов. В [@donobo3] предлагается следующая мера:

\begin{equation}\label{eq:sparsity}
 \nu_s(\boldsymbol x) = \frac{s^{-1}\sum_{i=1}^{s}(x^2_i - 1)}{s^{-1/2}\log_2^{3/2}(s)}
 = s^{-1/2} \frac{\sum_{i=1}^{s}(x^2_i-1)}{\log_2^{3/2}(s)}
\end{equation}

где, $\mathbf{x}$ предполагается разреженным, если $\nu_s(\mathbf{x}) \leq 1$ и неразреженным в противном случае. Окончательно, правило выбора порогового значения для зашумленных данных используя SureShrink определяется следующим образом:

\begin{equation}\label{eq:sureshrink}
\lambda_{SureShrink}(\mathbf{x}) = \begin{cases}
  \sqrt{2\log s}, ~ \nu_s(\mathbf{x}) \leq 1 \\
  \lambda^*, ~ \nu_s{\mathbf{x}} > 1
  \end{cases}
\end{equation}

В нашем случае $\mathbf{x}$ на самом деле соответствует набору детальных коэффициентов $\mathbf{d}_j$, где $j$ указывает уровень разложения. Тогда $lambda$ есть функция зависящая от уровня $j$: $\lambda = \lambda(j)$

## NeighBlock
Метод применяемый в SureShrink достигает хорошей преспосабливаемости к данным, учитывающей разреженность коэффициентов. Однако, он не принимает во внимание локальную близость каждого коэффициента, и решение принимается для каждого уровня. Что в некоторых случаях может привести к тому, что при применении метода будет удалено (срезано) слишком много коэффициентов.

Возможным решением увеличивающем оценку точности является использование информации о соседстве коэффициентов. Например, пороговое значение может быть выбрано для группы (блока) коэффициентов, исходя из их расположения. Где каждая группа содержит подмножество коэффициентов на данном уровне.

Желание работать с блоками коэффициентов объясняется просто: если соседний коэффициент является важной частью сигнала, тогда достаточно вероятно, что текущий сигнал так же является важным и тогда будет использовано меньшее пороговое значение. Что приводит нас к выгодному выбору между шумом и сигналом.

Кали и Сильверман в [@silver7] предложили метод названный ими __NeighBlock__, который использует информацию о соседних коэффициентах при принятии решения. Этот метод состоит из следующих шагов.

* На каждом уровне детализации $j$ вейвлет коэффициенты группируются в непересекающиеся блоки $(jb)$ длинны $L_0 = [\frac{\log n}{2}]$.

* Каждый блок расширяется с каждой стороны на длину $L_1 = max(1, [L_0 / 2])$ в каждом направлении, формируя пересекающиеся блоки $(jB)$ длины $L=L_0 + 2L_1$

Теперь коэффициенты для каждого непересекающегося блока $jb$ оцениваются следующим выражением:

\begin{equation}\label{eq:jbest}
\hat{d}_{jk}^{(jb)} = max(0, \frac{S^2_{jB} - \lambda_CL}{S^{jB}}) d_{jk}^{(jb)}
\end{equation}

где $S^2_i$ обозначает скейлограмму $i$-ого блока (сумму квадратов коэффициентов), а $\lambda_C$ выбирается как решение уравнения $\lambda_C - \log \lambda_C = 3$ и равно $\lambda_C = 4.505$. Выбор такого значения аналогичен выбору универсального граничного значения $\lambda_U$ и объясняется следующим образом.

Пусть $X_1, \dots, X_s$ назависимы и одинаково распределены с нормальным законом $\mathcal{N}(0, 1)$ и $L = log s$. Разделим $X_i$ на блоки длинны $L$. Тогда скейлограмма блока удовлетворяет[@cai]:

\begin{equation}\label{eq:lambdacprob}
P\{\underset{b}{max} S_b^2 > \lambda_CL\} \overset{\rightarrow}{s \rightarrow \infty} 0
\end{equation}

где $\lambda_C$ есть наименьшая константа удовлетворяющее \ref{eq:lambdacprob}. То есть при выборе $\lambda_C$ чистый шум должен быть полностью убран, с достаточно большой вероятностью.

## Сравнение методов SureShrink и NeighBlock


